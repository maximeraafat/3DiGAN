{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "weKUr0Ckkv2S"
   },
   "outputs": [],
   "source": [
    "### Mount google drive if available\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    drive_path = '/content/drive/MyDrive/term_paper/'\n",
    "    in_colab = True\n",
    "except:\n",
    "    drive_path = ''\n",
    "    in_colab = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VgJYmow2NB91"
   },
   "outputs": [],
   "source": [
    "### Install all dependecies\n",
    "\n",
    "# open3d\n",
    "need_open3d=False\n",
    "try:\n",
    "    import open3d\n",
    "except ModuleNotFoundError:\n",
    "    need_open3d=True\n",
    "\n",
    "if need_open3d:\n",
    "    !pip install open3d\n",
    "\n",
    "\n",
    "# pytorch3d\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "need_pytorch3d=False\n",
    "try:\n",
    "    import pytorch3d\n",
    "except ModuleNotFoundError:\n",
    "    need_pytorch3d=True\n",
    "\n",
    "if need_pytorch3d:\n",
    "    if torch.__version__.startswith(\"1.10.\") and sys.platform.startswith(\"linux\"):\n",
    "        # We try to install PyTorch3D via a released wheel.\n",
    "        pyt_version_str=torch.__version__.split(\"+\")[0].replace(\".\", \"\")\n",
    "        version_str=\"\".join([\n",
    "            f\"py3{sys.version_info.minor}_cu\",\n",
    "            torch.version.cuda.replace(\".\",\"\"),\n",
    "            f\"_pyt{pyt_version_str}\"\n",
    "        ])\n",
    "        !pip install pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html\n",
    "    else:\n",
    "        # We try to install PyTorch3D from source.\n",
    "        !curl -LO https://github.com/NVIDIA/cub/archive/1.10.0.tar.gz\n",
    "        !tar xzf 1.10.0.tar.gz\n",
    "        os.environ[\"CUB_HOME\"] = os.getcwd() + \"/cub-1.10.0\"\n",
    "        !pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'\n",
    "\n",
    "\n",
    "# cleanup\n",
    "!rm -rf 1.10.0.tar.gz cub-1.10.0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8owkzySKcO6c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "#import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision.io import read_image\n",
    "from typing import List, Union\n",
    "\n",
    "from pytorch3d.io import load_obj\n",
    "from pytorch3d.vis.plotly_vis import plot_scene, plot_batch_individually\n",
    "from pytorch3d.ops import sample_points_from_meshes\n",
    "\n",
    "from pytorch3d.structures import (\n",
    "    Meshes,\n",
    "    Pointclouds,\n",
    "    packed_to_list\n",
    ")\n",
    "\n",
    "from pytorch3d.renderer import (\n",
    "    look_at_view_transform,\n",
    "    FoVOrthographicCameras,\n",
    "    PointsRasterizationSettings,\n",
    "    PointsRenderer,\n",
    "    PointsRasterizer,\n",
    "    AlphaCompositor,\n",
    "    RasterizationSettings,\n",
    "    TexturesUV,\n",
    "    TexturesVertex\n",
    ")\n",
    "\n",
    "from pytorch3d.loss import (\n",
    "    chamfer_distance,\n",
    "    mesh_edge_loss,\n",
    "    mesh_normal_consistency,\n",
    "    mesh_laplacian_smoothing,\n",
    "    point_mesh_edge_distance,\n",
    "    point_mesh_face_distance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "unsdLa2kUaEY"
   },
   "outputs": [],
   "source": [
    "### Setup\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "An7QX7X9T4CX"
   },
   "outputs": [],
   "source": [
    "### Download data for subject 1\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import urllib.request as request\n",
    "\n",
    "attributes = ['body_texture', 'pointcloud'] # ['body', 'body_texture', 'pointcloud']\n",
    "pointcloud_subjects = [[1, 80], [81, 140], [141, 220], [221, 300], [301, 380], [381, 453]]\n",
    "\n",
    "subject = 70\n",
    "\n",
    "# Determine pointcloud interval for current subject\n",
    "pointcloud_zip = 'subject_'\n",
    "for i in pointcloud_subjects:\n",
    "    if subject >= i[0] and subject <= i[1]:\n",
    "        pointcloud_zip += '%d_%d.zip' % (i[0], i[1])\n",
    "\n",
    "for attr in attributes:\n",
    "    if attr != 'pointcloud':\n",
    "        url = os.path.join('https://humbi-dataset.s3.amazonaws.com', attr + '_subject', 'subject_%d.zip' % subject)\n",
    "        path = '%s_subject_%d.zip' % (attr, subject)\n",
    "        request.urlretrieve(url, path)\n",
    "        downloaded_zip = zipfile.ZipFile(path)\n",
    "        downloaded_zip.extractall() # !unzip downloaded_zip\n",
    "        os.remove(path)\n",
    "\n",
    "    else:\n",
    "        pointcloud_url = 'https://humbi-dataset.s3.amazonaws.com/pointcloud/' + pointcloud_zip\n",
    "        pointcloud_path = 'pointcloud_' + pointcloud_zip\n",
    "        request.urlretrieve(pointcloud_url, pointcloud_path)\n",
    "        downloaded_zip = zipfile.ZipFile(pointcloud_path)\n",
    "        for filename in downloaded_zip.namelist():\n",
    "            if filename.startswith('subject_%d/' % subject):\n",
    "                downloaded_zip.extract(filename)\n",
    "        os.remove(pointcloud_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IYT9fX9HY8ae"
   },
   "outputs": [],
   "source": [
    "### Extract mesh vertices and texture for a specific subject and pose\n",
    "\n",
    "def extract_verts(subject:int, pose:str):\n",
    "    filename = 'subject_%d/body/%s/reconstruction/smpl_vertex.txt' % (subject, pose)\n",
    "    verts = torch.Tensor( np.loadtxt(filename) ).to(device).unsqueeze(0)\n",
    "    return verts\n",
    "\n",
    "\n",
    "def extract_texture(subject:int, path_to_textures:str):\n",
    "    filename = 'median_subject_%d.png' % subject\n",
    "    img_path = os.path.join(path_to_textures, filename)\n",
    "\n",
    "    image = read_image(img_path)\n",
    "    image = torch.moveaxis(image, 0, 2).unsqueeze(0).float() * 1.0/255\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OE7W8J0z87h1"
   },
   "outputs": [],
   "source": [
    "### Construct a pytorch3d meshes object list, optinally with texture\n",
    "\n",
    "# Returns a list of meshes containing a mesh for each selected poses for a subject\n",
    "\n",
    "def construct_mesh_list(subject:int, poses:List[str], default_mesh_path:str, path_to_textures:str=None):\n",
    "    default_mesh = load_obj(default_mesh_path, load_textures=False)\n",
    "    mesh_faces = default_mesh[1].verts_idx.to(device).unsqueeze(0)\n",
    "\n",
    "    if path_to_textures is not None:\n",
    "        verts_uvs = default_mesh[2].verts_uvs.to(device).unsqueeze(0)\n",
    "        faces_uvs = default_mesh[1].textures_idx.to(device).unsqueeze(0)\n",
    "\n",
    "    texture_uv = None\n",
    "    if path_to_textures is not None:\n",
    "        texture = extract_texture(subject, path_to_textures)\n",
    "\n",
    "        texture_uv = TexturesUV(maps=texture, faces_uvs=faces_uvs, verts_uvs=verts_uvs)\n",
    "\n",
    "    meshes = []\n",
    "    for pose in tqdm(poses):\n",
    "        mesh_verts = extract_verts(subject, pose)\n",
    "        mesh = Meshes(mesh_verts, mesh_faces, texture_uv)\n",
    "        meshes.append(mesh)\n",
    "\n",
    "    return meshes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0wANHM0ndte4"
   },
   "outputs": [],
   "source": [
    "### Construct pytorch3d pointclouds, optinally with normals and surface reconstruction\n",
    "\n",
    "# Returns a pointcloud with rgb values, another one with normals as color features\n",
    "# and a reconstructed mesh from a poisson surface reconstruction\n",
    "\n",
    "def construct_pointcloud(subject:int, pose:str, surface_reconstruction:bool=False):\n",
    "    filename = 'subject_%d/body/%s/reconstruction/surface_reconstruction.txt' % (subject, pose)\n",
    "\n",
    "    reconstruction = np.loadtxt(filename)\n",
    "\n",
    "    # open3d\n",
    "    #o3d_pointcloud = o3d.geometry.PointCloud()\n",
    "    #o3d_pointcloud.points = o3d.utility.Vector3dVector(reconstruction[:, :3])\n",
    "\n",
    "    # pytorch3d\n",
    "    p3d_points = torch.Tensor(reconstruction[:, :3]).to(device).unsqueeze(0)\n",
    "    p3d_rgb = torch.Tensor(reconstruction[:, 3:6]).to(device).unsqueeze(0) * 1.0/255\n",
    "\n",
    "    pointcloud = Pointclouds(points=p3d_points, features=p3d_rgb)\n",
    "\n",
    "    # surface reconstruction\n",
    "    if surface_reconstruction:\n",
    "        # compute normals\n",
    "        o3d_pointcloud.estimate_normals()\n",
    "        o3d_pointcloud.orient_normals_consistent_tangent_plane(100)\n",
    "\n",
    "        p3d_normals = torch.Tensor(o3d_pointcloud.normals).to(device).unsqueeze(0)\n",
    "        pointcloud_nrm = Pointclouds(points=p3d_points, features=p3d_normals)\n",
    "\n",
    "        # poisson surface reconstruction in open3d\n",
    "        o3d_mesh, densities = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(o3d_pointcloud, depth=9)\n",
    "\n",
    "        # remove low density vertices\n",
    "        densities = np.asarray(densities)\n",
    "        vertices_to_remove = densities < np.quantile(densities, 0.02)\n",
    "        o3d_mesh.remove_vertices_by_mask(vertices_to_remove)\n",
    "\n",
    "        # crop surface outside the bounding box\n",
    "        bbox = o3d_pointcloud.get_axis_aligned_bounding_box()\n",
    "        o3d_reconstructed_mesh = o3d_mesh.crop(bbox)\n",
    "\n",
    "        # pytorch3d reconstruction mesh\n",
    "        vertices = torch.Tensor(o3d_reconstructed_mesh.vertices).to(device).unsqueeze(0)\n",
    "        faces = torch.Tensor(o3d_reconstructed_mesh.triangles).to(device).unsqueeze(0)\n",
    "\n",
    "        reconstructed_mesh = Meshes(vertices, faces)\n",
    "\n",
    "        return pointcloud, pointcloud_nrm, reconstructed_mesh\n",
    "\n",
    "    return pointcloud, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ksBlOvGBUe6L"
   },
   "outputs": [],
   "source": [
    "### Construct pointcloud list for all specified poses of a subject\n",
    "\n",
    "# Returns a list containing the rgb pointclouds for each pose of a subject,\n",
    "# a second similar list containing pointclouds with normals as color features\n",
    "# and a list with surface reconstruction for each pose\n",
    "\n",
    "def pointcloud_list(subject:int, poses:List[str], surface_reconstruction:bool=False):\n",
    "    pointclouds = []\n",
    "    normals = []\n",
    "    reconstructions = []\n",
    "\n",
    "    if surface_reconstruction:\n",
    "        print('surface reconstruction can take a while...')\n",
    "\n",
    "    for pose in tqdm(poses):\n",
    "        poincloud, normal, reconstruction = construct_pointcloud(subject, pose, surface_reconstruction)\n",
    "        pointclouds.append(poincloud)\n",
    "        normals.append(normal)\n",
    "        reconstructions.append(reconstruction)\n",
    "\n",
    "    return pointclouds, normals, reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bdNcUQpDZdcY"
   },
   "outputs": [],
   "source": [
    "### Extract keypoint for specified pose and subject\n",
    "\n",
    "def extract_keypoint(subject:int, pose:str):\n",
    "    filename = 'subject_%d/body/%s/reconstruction/keypoints.txt' % (subject, pose)\n",
    "\n",
    "    keypoints = np.loadtxt(filename)\n",
    "\n",
    "    return torch.Tensor(keypoints).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z25rfUXOZeON"
   },
   "outputs": [],
   "source": [
    "### Construct keypoint list for all specified poses of a subject\n",
    "\n",
    "def keypoint_list(subject:int, poses:List[str]):\n",
    "\n",
    "    keypoints = []\n",
    "    for pose in poses:\n",
    "        keypoint = extract_keypoint(subject, pose)\n",
    "        keypoints.append(keypoint)\n",
    "\n",
    "    return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CTIOTyYUJ8UX"
   },
   "outputs": [],
   "source": [
    "### Get mapping from OpenPose joints to SMPL vertex indices\n",
    "\n",
    "def openpose_to_smpl_verts_idx(op_joints_map:dict, vertex_ids:dict):\n",
    "    correspondances = []\n",
    "\n",
    "    for key in op_joints_map:\n",
    "        if key in vertex_ids:\n",
    "            correspondances.append(vertex_ids[key])\n",
    "        else:\n",
    "            correspondances.append(None)\n",
    "\n",
    "    return correspondances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yJHEoWrJKBRH"
   },
   "outputs": [],
   "source": [
    "### SMPL vertex indices (extracted from OpenPose joints) to vertex positions for a mesh\n",
    "\n",
    "def keypoints_positions(vertex_correspondances:List[int], mesh:Meshes):\n",
    "    assert( len(mesh[0].verts_list()) == 1 ), 'mesh can only have one mesh object (one subject, one pose)'\n",
    "\n",
    "    openpose_keypoints = torch.Tensor().to(device)\n",
    "    for idx in vertex_correspondances:\n",
    "        if idx is None:\n",
    "            pos = [float('nan')] * 3\n",
    "            pos = torch.Tensor(pos).unsqueeze(0)\n",
    "        else:\n",
    "            pos = mesh.verts_padded()[:,idx]\n",
    "        openpose_keypoints = torch.cat( (openpose_keypoints, pos) )\n",
    "\n",
    "    return openpose_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fdQHD6JvKFGO"
   },
   "outputs": [],
   "source": [
    "### Loss for keypoints (euclidian norm of distances between respective keypoints)\n",
    "\n",
    "def keypoints_loss(gt_keypoints:torch.Tensor, src_keypoints:torch.Tensor):\n",
    "    assert(gt_keypoints.shape == src_keypoints.shape), 'gt_keypoints and src_keypoints must have the same shape'\n",
    "\n",
    "    nan_mask = (torch.isnan(gt_keypoints)[:, 0] == False)\n",
    "\n",
    "    gt_keypoints_masked = gt_keypoints[nan_mask]\n",
    "    src_keypoints_masked = src_keypoints[nan_mask]\n",
    "\n",
    "    return torch.linalg.norm(gt_keypoints_masked - src_keypoints_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0If4SHZkW3HP"
   },
   "outputs": [],
   "source": [
    "### Convert TexturesUV object to TexturesVertex object and convert meshes with TexturesUV to meshes with TexturesVertex functions\n",
    "\n",
    "# Returns a TexturesVertex object\n",
    "\n",
    "def convert_to_textureVertex(textures_uv:TexturesUV, meshes:Meshes) -> TexturesVertex:\n",
    "    verts_colors_packed = torch.zeros_like(meshes.verts_packed()).to(device)\n",
    "    verts_colors_packed[meshes.faces_packed()] = textures_uv.faces_verts_textures_packed().to(device)\n",
    "\n",
    "    return TexturesVertex( packed_to_list(verts_colors_packed, meshes.num_verts_per_mesh()) )\n",
    "\n",
    "# Returns a list of mesh with a TexturesVertex object instead of a TexturesUV object\n",
    "\n",
    "def convert_mesh_texture(meshes:List[Meshes]):\n",
    "    # nb_meshes = len(meshes.verts_list())\n",
    "    assert( isinstance(meshes.textures, TexturesUV) ), 'meshes texture needs to be of type TexturesUV'\n",
    "\n",
    "    for mesh in meshes:\n",
    "        verts_features = convert_to_textureVertex(meshes.textures, meshes)\n",
    "        mesh = Meshes(meshes.verts_list(), meshes.faces_list(), verts_features)\n",
    "\n",
    "    return meshes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NxI-4srThgge"
   },
   "outputs": [],
   "source": [
    "### Plot interactive scene for multiple meshes or pointclouds\n",
    "\n",
    "def plot_structure(structures:List[ Union[Meshes,Pointclouds] ]):\n",
    "    assert(bool(structures) != None), 'nothing to be plotted'\n",
    "\n",
    "    if not isinstance(structures, list):\n",
    "        structures_clone = [structures.clone()]\n",
    "    else:\n",
    "        structures_clone = []\n",
    "        for structure in structures:\n",
    "            structures_clone.append(structure.clone())\n",
    "\n",
    "    end = len(structures_clone)\n",
    "    offsets = torch.arange(0, end, step=1)\n",
    "\n",
    "    dict_string = []\n",
    "    for i, structure in enumerate(structures_clone):\n",
    "        offset = torch.Tensor( [offsets[i], 0, 0] ).to(device)\n",
    "        if isinstance(structure, Meshes):\n",
    "            structure.verts_list()[0] = structure.verts_list()[0] + offset\n",
    "            dict_string.append('mesh %d' % (i+1))\n",
    "        elif isinstance(structure, Pointclouds):\n",
    "            structure.points_list()[0] = structure.points_list()[0] + offset\n",
    "            dict_string.append('pointcloud %d' % (i+1))\n",
    "\n",
    "    zip_iterator = zip(dict_string, structures_clone)\n",
    "    plot_structures = {'PLOT': dict(zip_iterator)} \n",
    "\n",
    "    return plot_scene(plot_structures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mlUgCEQcKg8a"
   },
   "outputs": [],
   "source": [
    "### Map 25 OpenPose joints (in the order provided by OpenPose) to SMPL joints\n",
    "# See https://github.com/nkolot/SPIN/blob/master/constants.py\n",
    "# See https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/02_output.md\n",
    "\n",
    "op_joints_map = {\n",
    "    'Nose'     : 24, 'Neck'  : 12, 'RShoulder': 17, 'RElbow'   : 19, 'RWrist' : 21,\n",
    "    'LShoulder': 16, 'LElbow': 18, 'LWrist'   : 20, 'MidHip'   :  0, 'RHip'   :  2, \n",
    "    'RKnee'    :  5, 'RAnkle':  8, 'LHip'     :  1, 'LKnee'    :  4, 'LAnkle' :  7,\n",
    "    'REye'     : 25, 'LEye'  : 26, 'REar'     : 27, 'LEar'     : 28, 'LBigToe': 29,\n",
    "    'LSmallToe': 30, 'LHeel' : 31, 'RBigToe'  : 32, 'RSmallToe': 33, 'RHeel'  : 34\n",
    "}\n",
    "\n",
    "### SMPL joints to vertices mapping for surface level joints\n",
    "# See https://github.com/vchoutas/smplx/blob/master/smplx/vertex_ids.py\n",
    "\n",
    "smpl_vertex_ids = {\n",
    "    'Nose'   :  332, 'Reye'     : 6260, 'LEye'  : 2800,\n",
    "    'REar'   : 4071, 'LEar'     :  583, 'RThumb': 6191,\n",
    "    'RIndex' : 5782, 'RMiddle'  : 5905, 'RRing' : 6016,\n",
    "    'RPinky' : 6133, 'LThumb'   : 2746, 'LIndex': 2319,\n",
    "    'LMiddle': 2445, 'LRing'    : 2556, 'LPinky': 2673,\n",
    "    'LBigToe': 3216, 'LSmallToe': 3226, 'LHeel' : 3387,\n",
    "    'RBigToe': 6617, 'RSmallToe': 6624, 'RHeel' : 6787\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "VYu_EyKeHYRi"
   },
   "outputs": [],
   "source": [
    "### Construct and plot meshes, pointclouds and keypoints list for chosen subject\n",
    "\n",
    "subject = 70\n",
    "\n",
    "poses = []\n",
    "poses_path = 'subject_%d/body/' % subject\n",
    "for pose in sorted(os.listdir(poses_path)):\n",
    "    pose_path = os.path.join(poses_path, pose)\n",
    "    if os.path.isdir(pose_path):\n",
    "        poses.append(pose)\n",
    "\n",
    "obj_mesh = drive_path + 'smpl_bodies/text_uv_coor_smpl.obj'\n",
    "texture_path = drive_path + 'humbi_maps/humbi_body_texture/body_texture_medians/'\n",
    "\n",
    "\n",
    "# mesh\n",
    "try:\n",
    "    humbi_meshes\n",
    "except NameError:\n",
    "    humbi_meshes = construct_mesh_list(subject, poses, default_mesh_path=obj_mesh, path_to_textures=texture_path)\n",
    "\n",
    "for i, pose in enumerate(poses):\n",
    "    print('subject {:03d}, pose %s, mesh :'.format(subject) % pose, ':', humbi_meshes[i])\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "# pointcloud\n",
    "try:\n",
    "    humbi_pointclouds\n",
    "except NameError:\n",
    "    humbi_pointclouds, humbi_pointclouds_nrm, humbi_reconstructions = pointcloud_list(subject, poses, surface_reconstruction=False)\n",
    "\n",
    "for i, pose in enumerate(poses):\n",
    "    print('subject {:03d}, pose %s, rgb pointcloud'.format(subject) % pose, ':'.rjust(4), humbi_pointclouds[i])\n",
    "    print('subject {:03d}, pose %s, normal pointcloud'.format(subject) % pose, ':'.rjust(0), humbi_pointclouds_nrm[i])\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "# keypoints\n",
    "try:\n",
    "    humbi_keypoints\n",
    "except NameError:\n",
    "    humbi_keypoints = keypoint_list(subject, poses)\n",
    "\n",
    "smpl_verts_correspondances = openpose_to_smpl_verts_idx(op_joints_map, smpl_vertex_ids)\n",
    "\n",
    "# plotting\n",
    "plot_structure([humbi_meshes[0], humbi_pointclouds[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bAF2bxNp1ffg"
   },
   "outputs": [],
   "source": [
    "### Construct pointcloud rendered\n",
    "\n",
    "R, T = look_at_view_transform(20, 10, 0, up=((0, 0, 1),))\n",
    "\n",
    "cameras = FoVOrthographicCameras(device=device, R=R, T=T).to(device)\n",
    "\n",
    "raster_settings = PointsRasterizationSettings(\n",
    "    image_size=512, \n",
    "    radius = 0.003,\n",
    "    points_per_pixel = 10\n",
    ")\n",
    "\n",
    "rasterizer = PointsRasterizer(cameras=cameras, raster_settings=raster_settings)\n",
    "\n",
    "renderer = PointsRenderer(\n",
    "    rasterizer=rasterizer,\n",
    "    compositor=AlphaCompositor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6GePqOxN14nC"
   },
   "outputs": [],
   "source": [
    "### Visualize pointcloud reconstruction with rgb values\n",
    "rgb_image = renderer(humbi_pointclouds[0]).detach()\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(rgb_image[0, ..., :3].cpu().numpy())\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yLsoKr7H8zHw"
   },
   "outputs": [],
   "source": [
    "### Visualize pointcloud reconstruction with estimated normal values\n",
    "nrm_image = renderer(humbi_pointcloud_nrm[0]).detach()\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(nrm_image[0, ..., :3].cpu().numpy())\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0C-olTI_bUd0"
   },
   "outputs": [],
   "source": [
    "### Offset the vertices along their normals\n",
    "# See https://pytorch3d.readthedocs.io/en/latest/modules/structures.html#pytorch3d.structures.Meshes.offset_verts_\n",
    "\n",
    "def offset_verts_along_nrm(displace_normals:torch.Tensor, mesh:Meshes):\n",
    "    verts_packed = mesh.verts_packed()\n",
    "\n",
    "    if displace_normals.shape != verts_packed[:,0].shape:\n",
    "        raise ValueError(\"displace_normals must have dimension (#verts, 3).\")\n",
    "\n",
    "    # update verts packed\n",
    "    displacement = torch.Tensor().to(device)\n",
    "    for i in range(3):\n",
    "        displacement = torch.cat( (displacement, (displace_normals * mesh.verts_normals_packed()[:,i]).unsqueeze(0)) ).to(torch.float32)\n",
    "\n",
    "    mesh._verts_packed = verts_packed + torch.moveaxis(displacement, 0, 1)\n",
    "    new_verts_list = list( mesh._verts_packed.split(mesh.num_verts_per_mesh().tolist(), 0) )\n",
    "\n",
    "    # update verts list\n",
    "    mesh._verts_list = new_verts_list\n",
    "\n",
    "    # update verts padded\n",
    "    if mesh._verts_padded is not None:\n",
    "        for i, verts in enumerate(new_verts_list):\n",
    "            if len(verts) > 0:\n",
    "                mesh._verts_padded[i, : verts.shape[0], :] = verts\n",
    "    \n",
    "    # returning mesh itself leads to plotting issues and gradient issues (missing grad_fn problem),\n",
    "    # we thus construct a new mesh and return it\n",
    "    return Meshes(mesh.verts_list(), mesh.faces_list(), mesh.textures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BUUW8jRDbXoX"
   },
   "outputs": [],
   "source": [
    "### Draw n=sample_size random points from pointcloud and construct a new pointcloud from those randomly selected points\n",
    "\n",
    "def downsample_pointcloud(pointcloud:Pointclouds, sample_size:int):\n",
    "    shape = pointcloud.num_points_per_cloud().item()\n",
    "    indices = torch.randperm(shape)[:sample_size]\n",
    "\n",
    "    points = pointcloud.points_packed()[indices,:].unsqueeze(0)\n",
    "\n",
    "    normals = None\n",
    "    features = None\n",
    "\n",
    "    if pointcloud.normals_packed() is not None:\n",
    "        normals = pointcloud.normals_packed()[indices,:].unsqueeze(0)\n",
    "\n",
    "    if pointcloud.features_packed() is not None:\n",
    "        features = pointcloud.features_packed()[indices,:].unsqueeze(0)\n",
    "\n",
    "    downsampled_pointcloud = Pointclouds(points, normals, features)\n",
    "\n",
    "    return downsampled_pointcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AHyHlBtwbYpt"
   },
   "outputs": [],
   "source": [
    "### Fit mesh to pointcloud and reconstruction\n",
    "\n",
    "def fit_mesh(mesh:Meshes, pointcloud:Pointclouds, keypoint:torch.Tensor, displacement:torch.Tensor,\n",
    "             vertex_correspondances:List[int], iterations:int, optimizer,\n",
    "             w_chamfer=None, w_edge=None, w_laplace=None, w_normal=None,\n",
    "             w_pt_edge=None, w_pt_face=None, w_keypoints=None):\n",
    "\n",
    "    deformed_mesh = mesh.clone().detach()\n",
    "\n",
    "    loop = tqdm(range(iterations), total = iterations)\n",
    "\n",
    "    for i in loop:\n",
    "        optimizer.zero_grad() # initialize optimizer\n",
    "\n",
    "        # displace\n",
    "        deformed_mesh = offset_verts_along_nrm(displacement, deformed_mesh.detach())\n",
    "\n",
    "        loss = 0.0\n",
    "\n",
    "        pts_to_sample = deformed_mesh.num_verts_per_mesh().item()\n",
    "        sample_deformed_mesh = sample_points_from_meshes(deformed_mesh, pts_to_sample)\n",
    "\n",
    "        downsampled_pointcloud = downsample_pointcloud(pointcloud, pts_to_sample)\n",
    "\n",
    "        if w_chamfer is not None and w_chamfer != 0.0:\n",
    "            loss_chamfer, _ = chamfer_distance(downsampled_pointcloud, sample_deformed_mesh)\n",
    "            loss += w_chamfer * loss_chamfer\n",
    "\n",
    "        if w_edge is not None and w_edge != 0.0:\n",
    "            loss_edge = mesh_edge_loss(deformed_mesh)\n",
    "            loss += w_edge * loss_edge\n",
    "\n",
    "        if w_laplace is not None and w_laplace != 0.0:\n",
    "            loss_laplacian = mesh_laplacian_smoothing(deformed_mesh, method=\"uniform\")\n",
    "            loss += w_laplace * loss_laplacian\n",
    "\n",
    "        if w_normal is not None and w_normal != 0.0:\n",
    "            loss_normal = mesh_normal_consistency(deformed_mesh)\n",
    "            loss += w_normal * loss_normal\n",
    "\n",
    "        if w_pt_edge is not None and w_pt_edge != 0.0:\n",
    "            loss_pt_edge = point_mesh_edge_distance(deformed_mesh, downsampled_pointcloud)\n",
    "            loss += w_pt_edge * loss_pt_edge\n",
    "\n",
    "        if w_pt_face is not None and w_pt_face != 0.0:\n",
    "            loss_pt_face = point_mesh_face_distance(deformed_mesh, downsampled_pointcloud)\n",
    "            loss += w_pt_face * loss_pt_face\n",
    "\n",
    "        '''\n",
    "        if w_norm is not None and w_pt_face != 0.0:\n",
    "            loss_norm = torch.linalg.norm(displacement)\n",
    "            loss += w_norm * loss_norm\n",
    "        '''\n",
    "\n",
    "        if w_keypoints is not None and w_pt_face != 0.0:\n",
    "            gt_keypoints = keypoints_positions(vertex_correspondances, deformed_mesh)\n",
    "            loss_keypoints = keypoints_loss(gt_keypoints, keypoint)\n",
    "            loss += w_keypoints * loss_keypoints\n",
    "        \n",
    "        loop.set_description('total_loss = %.6f' % loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return displacement, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EgFSci6AbdHY"
   },
   "outputs": [],
   "source": [
    "def fit_meshes(meshes:List[Meshes], pointclouds:List[Pointclouds], keypoints:List[torch.Tensor],\n",
    "               vertex_correspondances:List[int], iterations_per_subject:int,\n",
    "               w_norm=0.0, w_chamfer=None, w_edge=None, w_laplace=None,\n",
    "               w_normal=None, w_pt_edge=None, w_pt_face=None, w_keypoints=None):\n",
    "\n",
    "    assert(len(meshes) == len(pointclouds)), 'meshes and pointclouds lists should be of same length'\n",
    "\n",
    "    displace_normals = torch.full([meshes[0].num_verts_per_mesh()], 0.0, device=device, requires_grad=True)\n",
    "\n",
    "    optimizer = torch.optim.Adam([displace_normals], lr=0.01, weight_decay=w_norm)\n",
    "\n",
    "    displacements = []\n",
    "    losses = []\n",
    "\n",
    "    for i, (mesh, pointcloud, keypoint) in enumerate(zip(meshes, pointclouds, keypoints)):\n",
    "        print( 'fit mesh %d out of %d' % (i+1, len(meshes)) )\n",
    "        displace_normals, loss = fit_mesh(mesh, pointcloud, keypoint, displace_normals, vertex_correspondances, iterations_per_subject, optimizer, w_chamfer, w_edge, w_laplace, w_normal, w_pt_edge, w_pt_face, w_keypoints)\n",
    "        displacements.append(displace_normals)\n",
    "        losses.append(loss)\n",
    "\n",
    "    return displacements, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VwUXwqVbbfLX"
   },
   "outputs": [],
   "source": [
    "### Fit humbi_meshes to humbi_pointclouds for considered subject\n",
    "fitted_displacements, losses = fit_meshes(humbi_meshes[0], humbi_pointclouds[0], humbi_keypoints,\n",
    "                                          smpl_verts_correspondances, iterations_per_subject=1000,\n",
    "                                          w_norm=1.0, w_chamfer=1.0, w_edge=1.0, w_laplace=0.1,\n",
    "                                          w_normal=0.1, w_pt_edge=0.0, w_pt_face=0.0, w_keypoints=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eR0pF-GEbhFe"
   },
   "outputs": [],
   "source": [
    "### Find fitted mesh with lowest error and plot the corresponding displacements onto first mesh\n",
    "\n",
    "index = 0 # plot pose with index\n",
    "min_idx = np.argmin(losses) # pose with minimal loss\n",
    "\n",
    "deformed_mesh = humbi_meshes[index].clone()\n",
    "deformed_mesh = offset_verts_along_nrm(fitted_displacements[min_idx], deformed_mesh)\n",
    "\n",
    "plot_structure([humbi_meshes[index], deformed_mesh, humbi_pointclouds[index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VKB9tkK7blBu"
   },
   "outputs": [],
   "source": [
    "### BPS\n",
    "\n",
    "need_bps=False\n",
    "try:\n",
    "    import bps\n",
    "except ModuleNotFoundError:\n",
    "    need_bps=True\n",
    "\n",
    "if need_bps:\n",
    "    !pip install git+https://github.com/sergeyprokudin/bps\n",
    "\n",
    "\n",
    "from bps import bps\n",
    "\n",
    "\n",
    "# Average displacements\n",
    "def average_displacements(meshes:List[Meshes], pointclouds:List[Pointclouds]):\n",
    "\n",
    "    avg_displ = torch.Tensor().to(device)\n",
    "\n",
    "    for mesh, pointcloud in zip(meshes, pointclouds):\n",
    "        displ = displace_verts(mesh.clone(), pointcloud.clone())\n",
    "        avg_displ = torch.cat((avg_displ, displ.unsqueeze(0)))\n",
    "\n",
    "    return torch.mean(avg_displ, dim=0)\n",
    "\n",
    "\n",
    "# Displace each vertex from a subject by lambda, where lambda = projection of vector to closest point to normal vector of considered vertex\n",
    "def displace_verts(mesh:Meshes, pointcloud:Pointclouds):\n",
    "    points = pointcloud.points_packed().unsqueeze(0).detach().cpu().numpy()\n",
    "\n",
    "    verts = mesh.verts_packed().detach().cpu().numpy()\n",
    "    vert_normals = mesh.verts_normals_packed()\n",
    "\n",
    "    displacements = bps.encode(points, bps_arrangement='custom', custom_basis=verts, bps_cell_type='deltas')\n",
    "    displacements = torch.Tensor(displacements).squeeze().to(device)\n",
    "\n",
    "    displacement_along_nrm = torch.sum(displacements * vert_normals, dim=1).to(device)\n",
    "\n",
    "    return displacement_along_nrm\n",
    "\n",
    "\n",
    "# Test displacements along normals with nearest points in pointcloud (bps)\n",
    "try:\n",
    "    avg_displacements\n",
    "except NameError:\n",
    "    avg_displacements = average_displacements(humbi_meshes, humbi_pointclouds)\n",
    "\n",
    "index = 0\n",
    "deformed_mesh = humbi_meshes[index].clone()\n",
    "deformed_mesh = offset_verts_along_nrm(avg_displacements, deformed_mesh)\n",
    "\n",
    "plot_structure(deformed_mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sQfmDOxLbu7a"
   },
   "outputs": [],
   "source": [
    "### Extract vertex uv pixel positions on a 2D square map\n",
    "# See https://github.com/facebookresearch/pytorch3d/discussions/588\n",
    "\n",
    "def verts_uvs_positions(nb_verts:int, default_mesh_path:str, map_size:int=1024):\n",
    "    default_mesh = load_obj(default_mesh_path, load_textures=False)\n",
    "\n",
    "    flatten_verts_idx = default_mesh[1].verts_idx.flatten()\n",
    "    flatten_textures_idx = default_mesh[1].textures_idx.flatten()\n",
    "    verts_uvs = default_mesh[2].verts_uvs\n",
    "\n",
    "    verts_to_uv_index = torch.zeros(nb_verts, dtype=torch.int64)\n",
    "    verts_to_uv_index[flatten_verts_idx] = flatten_textures_idx\n",
    "    verts_to_uvs = verts_uvs[verts_to_uv_index]\n",
    "\n",
    "    uv_x = ( float(map_size) * verts_to_uvs[:,0] ).unsqueeze(0)\n",
    "    uv_y = ( float(map_size) * (1.0 - verts_to_uvs[:,1]) ).unsqueeze(0)\n",
    "    verts_uvs_positions = torch.cat((uv_x, uv_y)).moveaxis(0,1).round()\n",
    "\n",
    "    return verts_uvs_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KL6-XmEybzya"
   },
   "outputs": [],
   "source": [
    "### Create displacement map for each vertex and perform interpolation (inpainint) between vertex values\n",
    "\n",
    "def inpainted_displacements(subject:int, displacements:torch.Tensor, verts_uvs_positions:torch.Tensor, path_to_textures:str=None,\n",
    "                            map_size:int=1024, inpainting_method='telea', return_texture:bool=False):\n",
    "\n",
    "    assert(inpainting_method == 'telea' or inpainting_method == 'ns'), \"inpainting_method must be one of 'telea' or 'ns' (Navier-Stokes)\"\n",
    "    \n",
    "    displacement_map = torch.full((map_size,map_size), 255.0/2.0, dtype=torch.uint8, device=device)\n",
    "    inpaint_mask = torch.ones((map_size,map_size), dtype=torch.uint8)\n",
    "\n",
    "    texture = read_image(path_to_textures + 'median_subject_%d.png' % subject)\n",
    "    texture = torch.moveaxis(texture, 0, 2)\n",
    "\n",
    "    mask_condition = (texture[:,:,0] == 0) & (texture[:,:,1] == 0) & (texture[:,:,2] == 0)\n",
    "    inpaint_mask[mask_condition] = 0\n",
    "\n",
    "    displacements_uint = displacements\n",
    "    if displacements.type() != 'torch.ByteTensor':\n",
    "        displacements_uint = (displacements * 255).round().type(torch.uint8)\n",
    "\n",
    "    for disp, pos in zip(displacements_uint, verts_uvs_positions):\n",
    "        pos_x = int( pos[0].item() )\n",
    "        pos_y = int( pos[1].item() )\n",
    "        displacement_map[pos_y, pos_x] = disp # pixels in our constructed displacement map which get a value\n",
    "        inpaint_mask[pos_y, pos_x] = 0 # pixels that are not inpainted\n",
    "\n",
    "    displacement_map[mask_condition] = 255.0/2.0\n",
    "\n",
    "    # inpainting\n",
    "    method = cv2.INPAINT_TELEA * (inpainting_method=='telea') + cv2.INPAINT_NS * (inpainting_method=='ns')\n",
    "    inpainted_displacements = cv2.inpaint(displacement_map.numpy(), inpaint_mask.numpy(), 3, method)\n",
    "\n",
    "    if return_texture:\n",
    "        return torch.Tensor(inpainted_displacements), displacement_map, inpaint_mask, texture\n",
    "    else:\n",
    "        return torch.Tensor(inpainted_displacements), displacement_map, inpaint_mask, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9f4a6vn5b7WN"
   },
   "outputs": [],
   "source": [
    "### Test displacements inpainting\n",
    "\n",
    "nb_verts = fitted_displacements[min_idx].size(dim=0)\n",
    "verts_uvs = verts_uvs_positions(nb_verts, obj_mesh)\n",
    "\n",
    "inpainted_displacements, displacement_map, inpaint_mask, texture = inpainted_displacements(subject, fitted_displacements[min_idx], verts_uvs, texture_path, return_texture=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oOH6rBMab7_B"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(texture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OImQFAppb_4k"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(inpaint_mask, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzcyEJ7TcB2X"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(displacement_map, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UTYqgJvncCRW"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(inpainted_displacements, cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "pointcloud_rendering.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
